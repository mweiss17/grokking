{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.grokking.dataset import ModularArithmetic, ModularArithmeticDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 3,  0, 54,  1],\n",
      "        [28,  0, 86,  1],\n",
      "        [50,  0, 90,  1],\n",
      "        ...,\n",
      "        [84,  0, 95,  1],\n",
      "        [78,  0, 43,  1],\n",
      "        [ 2,  0, 63,  1]]), tensor([48, 41, 59, 21, 97, 66, 43, 14, 63, 53, 12, 76, 32, 49, 34, 28, 63, 67,\n",
      "        75, 62, 68, 67, 35, 92, 59, 16, 58, 61,  6, 59, 55,  2, 87, 93, 46, 13,\n",
      "        37, 28, 85, 72, 76, 63, 63, 62, 86, 33, 93, 12, 11, 96, 41, 45, 51, 53,\n",
      "        88, 83, 89, 68, 31, 88, 61, 16, 44, 34, 48, 13, 55, 59, 71, 27, 20,  6,\n",
      "        52,  2, 82, 25, 59, 96, 44,  6, 46,  5, 98, 18, 73, 28, 28,  9, 82, 84,\n",
      "        16, 39,  4, 10, 76,  7, 71, 62, 83, 53,  9, 45, 98, 57, 86,  9, 31, 54,\n",
      "        10, 96, 81, 37, 23, 31, 11, 84, 77, 44, 41, 86, 25,  7,  6, 77, 68, 14,\n",
      "         2, 84, 80, 56, 15, 50, 22, 20, 81, 18, 89, 63, 37, 89,  5, 48, 14, 92,\n",
      "        34, 98, 76, 85, 86, 59, 52,  7, 63, 67, 83, 97, 51, 70, 13, 98, 41, 54,\n",
      "        60, 24, 62, 98, 83, 59,  6, 25,  9, 15, 86, 14, 88, 23, 58, 65, 50, 37,\n",
      "         2, 94, 55, 22, 86, 78, 85, 51, 19, 83, 92, 53, 39, 54,  9, 72, 94, 39,\n",
      "        88, 35, 65, 51, 92, 40, 79, 53, 10, 26, 63, 73, 36, 52, 57,  4, 60, 40,\n",
      "        27, 14, 60, 64, 50, 87, 82, 21, 88, 33, 65, 56, 40, 95, 85, 87, 15, 59,\n",
      "        75, 67, 84, 41, 77, 46, 24, 28, 46, 88, 83, 96, 26, 17, 85, 18, 23, 90,\n",
      "        53, 27, 98, 62, 20, 39, 33, 42,  8, 42, 90, 18, 30, 75, 37, 12, 97, 39,\n",
      "        64,  5, 82, 58, 88,  6,  3, 20, 45, 37, 77, 34, 28, 27, 97, 75, 87, 23,\n",
      "        84, 86, 15, 23, 86, 82, 56, 66,  4, 30, 24, 10, 55,  5,  6, 82, 63, 28,\n",
      "        18, 29, 10, 26, 17, 44, 72, 45, 63, 60, 21, 34, 20, 25, 89, 96, 60, 62,\n",
      "        51, 73, 60, 53, 81, 67, 15, 95, 95, 51, 41, 41,  3, 31, 29, 55, 10, 96,\n",
      "        48, 89, 80, 78, 80, 58, 61, 80, 27,  7, 15, 94, 42, 93, 42, 82, 76, 67,\n",
      "        46, 56, 91,  2,  2, 34, 89, 46,  5, 77, 88,  4, 85, 61,  6,  5, 70, 40,\n",
      "        67, 69, 67, 45, 48, 55, 38,  6, 54, 36, 84, 34, 51,  8, 34, 28, 38, 81,\n",
      "        32, 59, 65, 84, 47, 45,  7, 16, 83, 93, 23, 68, 65, 15, 90, 55, 79, 34,\n",
      "        58, 33, 77, 23, 47, 53, 81, 33, 23, 66, 62,  4, 55,  4, 97, 86, 17, 45,\n",
      "        61, 22, 91, 88, 79, 85, 28, 83, 78, 10, 86, 73, 59, 75, 22, 12, 70, 43,\n",
      "        85, 85, 74, 58, 76, 58, 29, 97, 34, 89, 70, 62, 15,  9, 68,  9, 54, 26,\n",
      "        48, 98, 30, 74, 16, 64, 29, 13, 51, 69, 59, 89, 42,  6, 92, 63, 30, 77,\n",
      "        55, 45, 76, 66, 61,  7, 45, 92, 98, 53, 43, 36, 77, 39, 76,  4, 38, 72,\n",
      "        22, 92,  9, 84, 98, 88, 37, 38]))\n"
     ]
    }
   ],
   "source": [
    "ma = ModularArithmetic()\n",
    "train_ds = ModularArithmeticDataset(ma, train=True)\n",
    "valid_ds = ModularArithmeticDataset(ma, train=False)\n",
    "batch_size = 512\n",
    "train_dataloader = DataLoader(train_ds, shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
    "print(next(iter(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        decoder_layers = TransformerDecoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, memory: Tensor) -> Tensor:\n",
    "        \n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)        \n",
    "        \n",
    "\n",
    "#         src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "#         src = self.pos_encoder(src)\n",
    "#         output = self.transformer_decoder(tgt, memory, tgt_mask, memory_mask)\n",
    "        \n",
    "#         output = self.decoder(output)\n",
    "#         return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 99  # size of vocabulary\n",
    "emsize = 128  # embedding dimension\n",
    "d_hid = 128  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.0  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Embedding(99, 128)\n",
       "  (decoder): Linear(in_features=128, out_features=99, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.0001  # learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = generate_square_subsequent_mask(4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
