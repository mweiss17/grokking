batch_size: 512
lr: 0.001
ntokens: 100  # size of vocabulary
emsize: 128  # embedding dimension
d_hid: 128  # dimension of the feedforward network model in nn.TransformerEncoder
nlayers: 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder
nhead: 4  # number of heads in nn.MultiheadAttention
dropout: 0.0  # dropout probability
weight_decay: 1.0
use_wandb: True
n_epochs: 10000
print_every: 100
experiment_directory: /home/hattie/scratch/grokking/modular_substraction
save_every: 100 #do not save model checkpoints if save_every == 0
seed: 1
dataset_name: 'subtraction_mod_97'
dataset_path: 'datasets'